{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Artificial Neural Networks for Image Recognition\n",
    "\n",
    "Let's cut through the hype and get our hands dirty with some neural network.\n",
    "\n",
    "## Modeling complex functions with artificial neural networks\n",
    "\n",
    "We might want to start with a recap about the **Perceptron** and **single layer neural networks**\n",
    "\n",
    "### Single-layer neural networks\n",
    "\n",
    "Let's remember the **Adaline** algorithm: we used **gradient descent** to train the algorithm and learn the weights for classification, in every epoch we update the weight vector $w$ using the following rule:\n",
    "\n",
    "$$\n",
    "w := w + \\Delta w, where \\Delta w = \\eta \\nabla J(w)\n",
    "$$\n",
    "\n",
    "What we did was to calculate the gradient based on the whole training set and updated the weights of the model by taking a step in the opposite direction of the gradient $\\nabla J(w)$. In order to find the optimal weights we optimize the **SSE** cost function $J(w)$, furthermore we multiply the gradient by the learning rate $\\eta$ which we choose to balance the speed of learning and avoiding to overshoot the global minimum.\n",
    "\n",
    "### Introducing the multi-layer neural network\n",
    "\n",
    "Here we will learn about **multi-layer feed forward neural network** which is also called **multi-layer perceptron (MLP)**. Below we can see a scheme of an MLP with three layers: one *input layer*, one *hidden layer* and one *output layer*. Units in the hidden layer are fully connected to the input layer and the same goes with the output layer and the hidden layer. If the network has more than one hidden layer we call it **deep artificial neural network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images/mlp.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f5f70ffcafb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./images/mlp.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/alanmarazzi/anaconda3/envs/python-ml/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alanmarazzi/anaconda3/envs/python-ml/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alanmarazzi/anaconda3/envs/python-ml/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/alanmarazzi/anaconda3/envs/python-ml/lib/python3.6/site-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images/mlp.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "Image(filename='./images/mlp.png', width=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote the $i$th activation unit in the $l$th layer as $a_{i}^{(l)}$, and the activation units $a_{0}^{(1)}$ and $a_{0}^{(2)}$ are the bias units that we set equal to 1\n",
    "\n",
    "### Activating a neural network via forward propagation\n",
    "\n",
    "To understand how **forward propagation** works let's summarize the MLP learning procedure:\n",
    "\n",
    "1. Starting at the input layer we forward propagate the patterns of the data through the network to generate an output\n",
    "2. Based on the network's output, we calculate the error that we want to minimize \n",
    "3. We backpropagate the error, find its derivative with respect to each weight and update the model\n",
    "\n",
    "After repeating the steps for multiple epochs and learning the wights we use forward propagation to calculate the output of the network and apply a threshold function to obtain class labels.\n",
    "\n",
    "## Classifying handwritten digits\n",
    "\n",
    "In this section we will train our multi-layer neural network to classify handwritten digits from **MNIST**.\n",
    "\n",
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    labels_path = os.path.join(path, '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, '%s-images-idx3-ubyte' % kind)\n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, dtype=np.uint8)\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, dtype=np.uint8).reshape(len(labels), 784)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **load_mnist** function returns two arrays: the first is an $n\\times m$ dimensional array (images) where $n$ is the number of samples and $m$ is the number of features. The training dataset has 60.000 digits and the test set contains 10.000 samples.\n",
    "\n",
    "Images consist of 28x28 pixels and each pixel is represented by a gray scale intensity value. Here we unroll the pixels onto a 1D vector representing the rows of our matrix. The second array contains **labels**, as to say the target variable.\n",
    "\n",
    "Let's load the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 60000, cols: 784\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = load_mnist('mnist', kind='train')\n",
    "print('Rows: %d, cols: %d' % (x_train.shape[0], x_train.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, cols: 784\n"
     ]
    }
   ],
   "source": [
    "x_test, y_test = load_mnist('mnist', kind='t10k')\n",
    "print('Rows: %d, cols: %d' % (x_test.shape[0], x_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the digits we are going to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29535495f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(10):\n",
    "    img = x_train[y_train == i][0].reshape(28, 28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How different are all the digits within a class? Let's plot some of the \"7\" that we can find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAADuCAYAAAAOR30qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f294c99c0f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=5,\n",
    "                       ncols=5,\n",
    "                       sharex=True,\n",
    "                       sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i in range(25):\n",
    "    img = x_train[y_train == 7][i].reshape(28,28)\n",
    "    ax[i].imshow(img, cmap='Greys', interpolation='nearest')\n",
    "\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import sys\n",
    "\n",
    "class NeuralNetMLP(object):\n",
    "    \"\"\"Feedforward neural network / Multi-layer perceptron classifier.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    n_output: int\n",
    "        Number of output units, should be equal to the number of unique \n",
    "        class labels.\n",
    "    n_features: int\n",
    "        Number of features in the target dataset. Should be equal to the\n",
    "        number of columns in the X array.\n",
    "    n_hidden: int (default: 30)\n",
    "        Number of hidden units.\n",
    "    l1: float (default: 0.0)\n",
    "        Lambda value for L1-regularization. No regularization with l1=0.0.\n",
    "    l2: float (default: 0.0)\n",
    "        Lambda value for L2-regularization. No regularization with l2=0.0.\n",
    "    epochs: int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "    eta: float (deafult: 0.001)\n",
    "        Learning rate.\n",
    "    alpha: float (default: 0.0)\n",
    "        Momentum constant. Factor multiplied with the gradient of the \n",
    "        previous epoch t-1 to improve learning speed:\n",
    "        w(t) := w(t) - (grad(t) + alpha*grad(t-1))\n",
    "    decrease_const: float (default: 0.0)\n",
    "        Decrease constant. Decreases learning rate after every epoch \n",
    "        via eta / (1 + epoch*decrease_const)\n",
    "    shuffle: bool (default: True)\n",
    "        Shuffles training data if True every epoch to avoid circles.\n",
    "    minibatches: int (default: 1)\n",
    "        Divides training data in k minibatches for efficiency.\n",
    "        Normal gradient descent if k=1.\n",
    "    random_state: int (deafult: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    \n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_: list\n",
    "        Sum of squared errors after every epoch.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=500, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=None):\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        \n",
    "    def _encode_labels(self, y, k):\n",
    "        \"\"\"Encode labels into one-hot representation.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        y:  array, shape = [n_samples]\n",
    "            Target values.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        onehot: array, shape = (n_labels, n_samples)\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        return onehot\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with random small numbers\"\"\"\n",
    "        w1 = np.random.uniform(-1.0, 1.0, size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = np.random.uniform(-1.0, 1.0, size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        return w1, w2\n",
    "    \n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "        \n",
    "        Uses scipy.special.expit to avoid overflow error for very small\n",
    "        values of z.\n",
    "        \n",
    "        \"\"\"\n",
    "        # return 1.0 / (1.0 + np.exp(-z))\n",
    "        return expit(z)\n",
    "    \n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "    \n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError(\"'how' must be 'column' or 'row'\")\n",
    "        return X_new\n",
    "    \n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "        \n",
    "        Parameters\n",
    "        ------------\n",
    "        X: array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1: array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer\n",
    "        w2: array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        a1: array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit\n",
    "        z2: array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer\n",
    "        a2: array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer\n",
    "        z3: array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer\n",
    "        a3: array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        return a1, z2, a2, z3, a3\n",
    "    \n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2 regularization cost\"\"\"\n",
    "        return (lambda_ / 2.0) * (np.sum(w1[:, 1:] ** 2) + \n",
    "                                  np.sum(w2[:, 1:] ** 2))\n",
    "    \n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1 regularization cost\"\"\"\n",
    "        return (lambda_ / 2.0) *(np.abs(w1[:, 1:]).sum() + \n",
    "                                 np.abs(w2[:, 1:]).sum())\n",
    "    \n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        y_enc: array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output: array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1: array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer\n",
    "        w2: array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        cost: float\n",
    "            Regularized cost\n",
    "            \n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        return cost\n",
    "    \n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\"Compute gradient step using backpropagation\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        a1: array, shape = [n_samples, n_features + 1]\n",
    "            Input values with bias unit\n",
    "        a2: array, shape = [n_hidden + 1, n_samples]\n",
    "            Activation of hidden layer\n",
    "        a3: array, shape = [n_output_units, n_samples]\n",
    "            Activvation of output layer\n",
    "        z2: array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer\n",
    "        y_enc: array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels\n",
    "        w1: array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer\n",
    "        w2: array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer\n",
    "            \n",
    "        Returns\n",
    "        --------\n",
    "        grad1: array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1\n",
    "        grad2: array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2\n",
    "        \n",
    "        \"\"\"\n",
    "        # Backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "        \n",
    "        # Regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "        \n",
    "        return grad1, grad2\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "        \n",
    "        Parameters\n",
    "        -----------\n",
    "        X: array, shape = [n_samples, n_features]\n",
    "            Input layer with original features\n",
    "        \n",
    "        Returns\n",
    "        --------\n",
    "        y_pred: array, shape = [n_samples]\n",
    "            Predicted class labels\n",
    "        \n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "            \n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\"Learn weights from training data\n",
    "        Parameters\n",
    "        -----------\n",
    "        X: array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y: array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress: bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        self\n",
    "        \n",
    "        \"\"\"\n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "        \n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "        \n",
    "        for i in range(self.epochs):\n",
    "            \n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "            \n",
    "            if print_progress:\n",
    "                sys.stderr.write('\\rEpoch: %d/%d' % (i+1, self.epochs))\n",
    "                sys.stderr.flush()\n",
    "            \n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "            \n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "                \n",
    "                # Compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "                \n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize a 784-50-10 MLP: a neural network with 784 input units (*n_features*), 50 hidden units (*n_hidden*) and 10 output units (*n_output*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetMLP(n_output=10,\n",
    "                  n_features=x_train.shape[1],\n",
    "                  n_hidden=50,\n",
    "                  l2=0.1,\n",
    "                  l1=0.0,\n",
    "                  epochs=1000,\n",
    "                  eta=0.001,\n",
    "                  alpha=0.001,\n",
    "                  decrease_const=0.00001,\n",
    "                  shuffle=True,\n",
    "                  minibatches=50,\n",
    "                  random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we train the MLP on 60.000 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 1000/1000"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NeuralNetMLP at 0x7f294c2dd5c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(x_train, y_train, print_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('nn.pickle', 'wb') as pick:\n",
    "    pickle.dump(nn, pick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
