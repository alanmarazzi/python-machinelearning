{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Model Evaluation and Hyperparameter Tuning\n",
    "\n",
    "## Streamlining workflows with pipelines\n",
    "\n",
    "When working on a dataset we saw that we have to reuse the parameters that were obtained during the fitting of the training data. We will learn how to use the **Pipeline** class in scikit-learn for this.\n",
    "\n",
    "### Loading a dataset\n",
    "\n",
    "We will be working with the **Breast Cancer Wisconsin** dataset which contains 569 samples of malignant and benign tumor cells.\n",
    "\n",
    "We will read the dataset and split it into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data', \n",
    "    header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we assign the 30 features to a NumPy array x, using **LabelEncoder** we transform the class labels from their original string representation (**M** and **B**) to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "x = df.loc[:, 2:].values\n",
    "y = df.loc[:, 1].values\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the two classes are encoded as 1 = M and 0 = B. To check if the behaviour is correct we can call the method on two dummy examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.transform(['M', 'B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before constructing the first model pipeline let's divide data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining transformers and estimators in a pipeline\n",
    "\n",
    "As we learned previously, we want to standardize the features of the model before running it. So, let's assume that we want to compress our data from 30 dimensions onto a lower two-dimensional subspace via PCA. Instead of going through the transformation steps we can chain the **StandardScaler**, **PCA** and **LogisticRegression** in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.947\n"
     ]
    }
   ],
   "source": [
    "pipe_lr = Pipeline([('scl', StandardScaler()), \n",
    "                    ('pca', PCA(n_components=2)), \n",
    "                    ('clf', LogisticRegression(random_state=1))])\n",
    "pipe_lr.fit(x_train, y_train)\n",
    "print('Test Accuracy: %.3f' % pipe_lr.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Pipeline** object takes a list of tuples as input where the first element is an ID used to access single elements in the pipeline, and the second element is a scikit-learn transformer or estimator.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "To avoid overfitting we can use **cross-validation**. There are two major techniques: **holdout cross-validation** and **k-fold cross-validation**.\n",
    "\n",
    "### Holdout method\n",
    "\n",
    "By splitting the dataset into training and test datasets and performing model selection and hyperparameter tuning on the same test set we make it become training data. A better way to use the **holdout method** is to split data into training, validation and test set.\n",
    "\n",
    "The issue with this method is that it will depend on the split of the data, and it can be impractical for small data problems.\n",
    "\n",
    "### K-fold cross-validation\n",
    "\n",
    "In **k-fold cross-validation** we randomly split the training dataset into *k* folds without replacement, where $k-1$ folds are used for model training and one fold is used for testing. The procedure is repeated *k* times. \n",
    "\n",
    "The default value for *k* is 10 which is a reasonable number for many datasets, for small ones we can increase *k*, in this way more data will be used for training every iteration.\n",
    "\n",
    "For classification problems we can improve k-fold performance with **Stratified k-fold** which tends to perform better with unequal class proportions. In fact with this method every fold retains the proportion of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1, Class dist.: [256 153], Acc: 0.891\n",
      "Fold: 2, Class dist.: [256 153], Acc: 0.978\n",
      "Fold: 3, Class dist.: [256 153], Acc: 0.978\n",
      "Fold: 4, Class dist.: [256 153], Acc: 0.913\n",
      "Fold: 5, Class dist.: [256 153], Acc: 0.935\n",
      "Fold: 6, Class dist.: [257 153], Acc: 0.978\n",
      "Fold: 7, Class dist.: [257 153], Acc: 0.933\n",
      "Fold: 8, Class dist.: [257 153], Acc: 0.956\n",
      "Fold: 9, Class dist.: [257 153], Acc: 0.978\n",
      "Fold: 10, Class dist.: [257 153], Acc: 0.956\n"
     ]
    }
   ],
   "source": [
    "kfold = StratifiedKFold(y=y_train,\n",
    "                        n_folds=10,\n",
    "                        random_state=1)\n",
    "scores = []\n",
    "for k, (train, test) in enumerate(kfold):\n",
    "    pipe_lr.fit(x_train[train], y_train[train])\n",
    "    score = pipe_lr.score(x_train[test], y_train[test])\n",
    "    scores.append(score)\n",
    "    print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k+1, np.bincount(y_train[train]), score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy: 0.950 +/- 0.029\n"
     ]
    }
   ],
   "source": [
    "print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the previous code is useful to understand how k-fold works under the hood, we have a better and faster implementation implemented in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy scores: [ 0.89130435  0.97826087  0.97826087  0.91304348  0.93478261  0.97777778\n",
      "  0.93333333  0.95555556  0.97777778  0.95555556]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(estimator=pipe_lr,\n",
    "                        X=x_train,\n",
    "                        y=y_train,\n",
    "                        cv=10,\n",
    "                        n_jobs=1)\n",
    "print('CV accuracy scores: %s' % scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV accuracy 0.950 +/- 0.029\n"
     ]
    }
   ],
   "source": [
    "print('CV accuracy %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging algorithms with learning and validation curves\n",
    "\n",
    "We will now take a look at the performance of algorithms via **learning curves** and **validation curves**.\n",
    "\n",
    "### Diagnosing bias and variance problems with learning curves\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
